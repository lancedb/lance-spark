{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":""},{"location":"#introduction","title":"Introduction","text":"<p>The Apache Spark Connector for Lance allows Apache Spark to efficiently read datasets stored in Lance format.</p> <p>Lance is a modern columnar data format optimized for machine learning workflows and datasets, supporting distributed, parallel scans, and optimizations such as column and filter pushdown to improve performance. Additionally, Lance provides high-performance random access that is 100 times faster than Parquet without sacrificing scan performance.</p> <p>By using the Apache Spark Connector for Lance, you can leverage Apache Spark's powerful data processing, SQL querying, and machine learning training capabilities on the AI data lake powered by Lance.</p>"},{"location":"#features","title":"Features","text":"<p>The connector is built using the Spark DatasourceV2 (DSv2) API. Please check this presentation to learn more about DSv2 features. Specifically, you can use the Apache Spark Connector for Lance to:</p> <ul> <li>Read &amp; Write Lance Datasets: Seamlessly read and write datasets stored in the Lance format using Spark.</li> <li>Distributed, Parallel Scans: Leverage Spark's distributed computing capabilities to perform parallel scans on Lance datasets.</li> <li>Column and Filter Pushdown: Optimize query performance by pushing down column selections and filters to the data source.</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#start-session","title":"Start Session","text":"PySparkSpark Shell (Scala) <pre><code>pyspark --packages com.lancedb:lance-spark-bundle-3.5_2.12:0.0.1\n</code></pre> <pre><code>spark-shell --packages com.lancedb:lance-spark-bundle-3.5_2.12:0.0.1\n</code></pre>"},{"location":"#prepare-data","title":"Prepare Data","text":"PythonScala <pre><code>from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType\n\nschema = StructType([\n    StructField(\"id\", IntegerType(), False),\n    StructField(\"name\", StringType(), False),\n    StructField(\"score\", FloatType(), False),\n])\n\ndata = [\n    (1, \"Alice\", 85.5),\n    (2, \"Bob\", 92.0),\n    (3, \"Carol\", 78.0),\n]\n\ndf = spark.createDataFrame(data, schema=schema)\n</code></pre> <pre><code>import org.apache.spark.sql.Row\nimport org.apache.spark.sql.types._\n\nval schema = StructType(Seq(\n  StructField(\"id\", IntegerType, nullable = false),\n  StructField(\"name\", StringType, nullable = false),\n  StructField(\"score\", FloatType, nullable = false)\n))\n\nval data = Seq(\n  Row(1, \"Alice\", 85.5f),\n  Row(2, \"Bob\", 92.0f),\n  Row(3, \"Carol\", 78.0f)\n)\n\nval rdd = spark.sparkContext.parallelize(data)\nval df = spark.createDataFrame(rdd, schema)\n</code></pre>"},{"location":"#simple-write","title":"Simple Write","text":"PythonScala <pre><code>(df.write\n    .format(\"lance\")\n    .mode(\"overwrite\")\n    .save(\"/tmp/manual_users.lance\"))\n</code></pre> <pre><code>df.write.\n    format(\"lance\").\n    mode(\"overwrite\").\n    save(\"/tmp/manual_users.lance\")\n</code></pre>"},{"location":"#simple-read","title":"Simple Read","text":"PythonScala <pre><code>(spark.read\n    .format(\"lance\")\n    .load(\"/tmp/manual_users.lance\")\n    .show())\n</code></pre> <pre><code>spark.read.\n    format(\"lance\").\n    load(\"/tmp/manual_users.lance\").\n    show()\n</code></pre>"},{"location":"#docker","title":"Docker","text":"<p>The project contains a docker image in the <code>docker</code> folder you can build and run a simple example notebook. To do so, clone the repo and run:</p> <pre><code>make docker-build\nmake docker-up\n</code></pre> <p>And then open the notebook at <code>http://localhost:8888</code>.</p>"},{"location":"dev-guide/","title":"Development Guide","text":""},{"location":"dev-guide/#lance-java-sdk-dependency","title":"Lance Java SDK Dependency","text":"<p>This package is dependent on the Lance Java SDK and Lance Namespace Java Modules. You need to build those repositories locally first before building this repository. If your have changes affecting those repositories, the PR in <code>lancedb/lance-spark</code> will only pass CI after the PRs in <code>lancedb/lance</code> and <code>lance/lance-namespace</code> are merged.</p>"},{"location":"dev-guide/#build-commands","title":"Build Commands","text":"<p>This connector is built using Maven. To build everything:</p> <pre><code>./mvnw clean install\n</code></pre> <p>To build everything without running tests:</p> <pre><code>./mvnw clean install -DskipTests\n</code></pre>"},{"location":"dev-guide/#multi-version-support","title":"Multi-Version Support","text":"<p>We offer the following build profiles for you to switch among different build versions:</p> <ul> <li>scala-2.12</li> <li>scala-2.13</li> <li>spark-3.4</li> <li>spark-3.5</li> </ul> <p>For example, to use Scala 2.13:</p> <pre><code>./mvnw clean install -Pscala-2.13\n</code></pre> <p>To build a specific version like Spark 3.4:</p> <pre><code>./mvnw clean install -Pspark-3.4\n</code></pre> <p>To build only Spark 3.4:</p> <pre><code>./mvnw clean install -Pspark-3.4 -pl lance-spark-3.4 -am\n</code></pre> <p>Use the <code>shade-jar</code> profile to create the jar with all dependencies for Spark 3.4:</p> <pre><code>./mvnw clean install -Pspark-3.4 -Pshade-jar -pl lance-spark-3.4 -am\n</code></pre>"},{"location":"dev-guide/#styling-guide","title":"Styling Guide","text":"<p>We use checkstyle and spotless to lint the code.</p> <p>To verify checkstyle:</p> <pre><code>./mvnw checkstyle:check\n</code></pre> <p>To verify spotless:</p> <pre><code>./mvnw spotless:check\n</code></pre> <p>To apply spotless changes:</p> <pre><code>./mvnw spotless:apply\n</code></pre>"},{"location":"dev-guide/#documentation-website","title":"Documentation Website","text":""},{"location":"dev-guide/#setup","title":"Setup","text":"<p>The documentation website is built using mkdocs-material. The easiest way to setup is to create a Python virtual environment and install the necessary dependencies:</p> <pre><code>python3 -m venv .env\nsource .env/bin/activate\npip install mkdocs-material\npip install mkdocs-awesome-pages-plugin\n</code></pre> <p>Then you can start the server at <code>http://localhost:8000/lance-spark</code> by:</p> <pre><code>source .env/bin/activate\nmkdocs serve\n</code></pre>"},{"location":"dev-guide/#contents","title":"Contents","text":"<p>In general, we push most of the contents in the website as the single source of truth. The welcome page is the same as the README of the GitHub repository. If you edit one of them, please make sure to update the other document.</p>"},{"location":"user-guide/config/","title":"Configuration","text":"<p>Spark DSV2 catalog integrates with Lance through Lance Namespace.</p>"},{"location":"user-guide/config/#basic-setup","title":"Basic Setup","text":"<p>Configure Spark with the <code>LanceNamespaceSparkCatalog</code> by setting the appropriate Spark catalog implementation  and namespace-specific options:</p> Parameter Type Required Description <code>spark.sql.catalog.{name}</code> String \u2713 Set to <code>com.lancedb.lance.spark.LanceNamespaceSparkCatalog</code> <code>spark.sql.catalog.{name}.impl</code> String \u2713 Namespace implementation, short name like <code>dir</code>, <code>rest</code>, <code>hive</code>, <code>glue</code> or full Java implementation class"},{"location":"user-guide/config/#example-namespace-implementations","title":"Example Namespace Implementations","text":""},{"location":"user-guide/config/#directory-namespace","title":"Directory Namespace","text":"ScalaJavaSpark ShellSpark Submit <pre><code>import org.apache.spark.sql.SparkSession\n\nval spark = SparkSession.builder()\n    .appName(\"lance-dir-example\")\n    .config(\"spark.sql.catalog.lance\", \"com.lancedb.lance.spark.LanceNamespaceSparkCatalog\")\n    .config(\"spark.sql.catalog.lance.impl\", \"dir\")\n    .config(\"spark.sql.catalog.lance.root\", \"/path/to/lance/database\")\n    .getOrCreate()\n</code></pre> <pre><code>import org.apache.spark.sql.SparkSession;\n\nSparkSession spark = SparkSession.builder()\n    .appName(\"lance-dir-example\")\n    .config(\"spark.sql.catalog.lance\", \"com.lancedb.lance.spark.LanceNamespaceSparkCatalog\")\n    .config(\"spark.sql.catalog.lance.impl\", \"dir\")\n    .config(\"spark.sql.catalog.lance.root\", \"/path/to/lance/database\")\n    .getOrCreate();\n</code></pre> <pre><code>spark-shell \\\n  --packages com.lancedb:lance-spark-bundle-3.5_2.12:0.0.1 \\\n  --conf spark.sql.catalog.lance=com.lancedb.lance.spark.LanceNamespaceSparkCatalog \\\n  --conf spark.sql.catalog.lance.impl=dir \\\n  --conf spark.sql.catalog.lance.root=/path/to/lance/database\n</code></pre> <pre><code>spark-submit \\\n  --packages com.lancedb:lance-spark-bundle-3.5_2.12:0.0.1 \\\n  --conf spark.sql.catalog.lance=com.lancedb.lance.spark.LanceNamespaceSparkCatalog \\\n  --conf spark.sql.catalog.lance.impl=dir \\\n  --conf spark.sql.catalog.lance.root=/path/to/lance/database \\\n  your-application.jar\n</code></pre>"},{"location":"user-guide/config/#directory-configuration-parameters","title":"Directory Configuration Parameters","text":"Parameter Required Description <code>spark.sql.catalog.{name}.root</code> \u2717 Storage root location (default: current directory) <code>spark.sql.catalog.{name}.storage.*</code> \u2717 Additional OpenDAL storage configuration options <code>spark.sql.catalog.{name}.extra_level</code> \u2717 Virtual level for 2-level namespaces (auto-set to <code>default</code>) <p>Example settings:</p> Local StorageS3MinIO <pre><code>from pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n    .appName(\"lance-dir-local-example\") \\\n    .config(\"spark.sql.catalog.lance\", \"com.lancedb.lance.spark.LanceNamespaceSparkCatalog\") \\\n    .config(\"spark.sql.catalog.lance.impl\", \"dir\") \\\n    .config(\"spark.sql.catalog.lance.root\", \"/path/to/lance/database\") \\\n    .getOrCreate()\n</code></pre> <pre><code>from pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n    .appName(\"lance-dir-minio-example\") \\\n    .config(\"spark.sql.catalog.lance\", \"com.lancedb.lance.spark.LanceNamespaceSparkCatalog\") \\\n    .config(\"spark.sql.catalog.lance.impl\", \"dir\") \\\n    .config(\"spark.sql.catalog.lance.root\", \"s3://bucket-name/lance-data\") \\\n    .config(\"spark.sql.catalog.lance.storage.access_key_id\", \"abc\") \\\n    .config(\"spark.sql.catalog.lance.storage.secret_access_key\", \"def\")\n    .config(\"spark.sql.catalog.lance.storage.session_token\", \"ghi\") \\\n    .getOrCreate()\n</code></pre> <pre><code>from pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n    .appName(\"lance-dir-minio-example\") \\\n    .config(\"spark.sql.catalog.lance\", \"com.lancedb.lance.spark.LanceNamespaceSparkCatalog\") \\\n    .config(\"spark.sql.catalog.lance.impl\", \"dir\") \\\n    .config(\"spark.sql.catalog.lance.root\", \"s3://bucket-name/lance-data\") \\\n    .config(\"spark.sql.catalog.lance.storage.endpoint\", \"http://minio:9000\") \\\n    .config(\"spark.sql.catalog.lance.storage.aws_allow_http\", \"true\") \\\n    .config(\"spark.sql.catalog.lance.storage.access_key_id\", \"admin\") \\\n    .config(\"spark.sql.catalog.lance.storage.secret_access_key\", \"password\") \\\n    .getOrCreate()\n</code></pre>"},{"location":"user-guide/config/#rest-namespace","title":"REST Namespace","text":"<p>Here we use LanceDB Cloud as an example of the REST namespace:</p> PySparkScalaJavaSpark ShellSpark Submit <pre><code>spark = SparkSession.builder \\\n    .appName(\"lance-rest-example\") \\\n    .config(\"spark.sql.catalog.lance\", \"com.lancedb.lance.spark.LanceNamespaceSparkCatalog\") \\\n    .config(\"spark.sql.catalog.lance.impl\", \"rest\") \\\n    .config(\"spark.sql.catalog.lance.headers.x-api-key\", \"your-api-key\") \\\n    .config(\"spark.sql.catalog.lance.headers.x-lancedb-database\", \"your-database\") \\\n    .config(\"spark.sql.catalog.lance.uri\", \"https://your-database.us-east-1.api.lancedb.com\") \\\n    .getOrCreate()\n</code></pre> <pre><code>val spark = SparkSession.builder()\n    .appName(\"lance-rest-example\")\n    .config(\"spark.sql.catalog.lance\", \"com.lancedb.lance.spark.LanceNamespaceSparkCatalog\")\n    .config(\"spark.sql.catalog.lance.impl\", \"rest\")\n    .config(\"spark.sql.catalog.lance.headers.x-api-key\", \"your-api-key\")\n    .config(\"spark.sql.catalog.lance.headers.x-lancedb-database\", \"your-database\")\n    .config(\"spark.sql.catalog.lance.uri\", \"https://your-database.us-east-1.api.lancedb.com\")\n    .getOrCreate()\n</code></pre> <pre><code>SparkSession spark = SparkSession.builder()\n    .appName(\"lance-rest-example\")\n    .config(\"spark.sql.catalog.lance\", \"com.lancedb.lance.spark.LanceNamespaceSparkCatalog\")\n    .config(\"spark.sql.catalog.lance.impl\", \"rest\")\n    .config(\"spark.sql.catalog.lance.headers.x-api-key\", \"your-api-key\")\n    .config(\"spark.sql.catalog.lance.headers.x-lancedb-database\", \"your-database\")\n    .config(\"spark.sql.catalog.lance.uri\", \"https://your-database.us-east-1.api.lancedb.com\")\n    .getOrCreate();\n</code></pre> <pre><code>spark-shell \\\n  --packages com.lancedb:lance-spark-bundle-3.5_2.12:0.0.1 \\\n  --conf spark.sql.catalog.lance=com.lancedb.lance.spark.LanceNamespaceSparkCatalog \\\n  --conf spark.sql.catalog.lance.impl=rest \\\n  --conf spark.sql.catalog.lance.headers.x-api-key=your-api-key \\\n  --conf spark.sql.catalog.lance.headers.x-lancedb-database=your-database \\\n  --conf spark.sql.catalog.lance.uri=https://your-database.us-east-1.api.lancedb.com\n</code></pre> <pre><code>spark-submit \\\n  --packages com.lancedb:lance-spark-bundle-3.5_2.12:0.0.1 \\\n  --conf spark.sql.catalog.lance=com.lancedb.lance.spark.LanceNamespaceSparkCatalog \\\n  --conf spark.sql.catalog.lance.impl=rest \\\n  --conf spark.sql.catalog.lance.headers.x-api-key=your-api-key \\\n  --conf spark.sql.catalog.lance.headers.x-lancedb-database=your-database \\\n  --conf spark.sql.catalog.lance.uri=https://your-database.us-east-1.api.lancedb.com \\\n  your-application.jar\n</code></pre>"},{"location":"user-guide/config/#rest-configuration-parameters","title":"REST Configuration Parameters","text":"Parameter Required Description <code>spark.sql.catalog.{name}.uri</code> \u2713 REST API endpoint URL (e.g., <code>https://api.lancedb.com</code>) <code>spark.sql.catalog.{name}.headers.*</code> \u2717 HTTP headers for authentication (e.g., <code>headers.x-api-key</code>)"},{"location":"user-guide/config/#aws-glue-namespace","title":"AWS Glue Namespace","text":"<p>AWS Glue is Amazon's managed metastore service that provides a centralized catalog for your data assets:</p> PySparkScalaJava <pre><code>spark = SparkSession.builder \\\n    .appName(\"lance-glue-example\") \\\n    .config(\"spark.sql.catalog.lance\", \"com.lancedb.lance.spark.LanceNamespaceSparkCatalog\") \\\n    .config(\"spark.sql.catalog.lance.impl\", \"glue\") \\\n    .config(\"spark.sql.catalog.lance.region\", \"us-east-1\") \\\n    .config(\"spark.sql.catalog.lance.catalog_id\", \"123456789012\") \\\n    .config(\"spark.sql.catalog.lance.access_key_id\", \"your-access-key\") \\\n    .config(\"spark.sql.catalog.lance.secret_access_key\", \"your-secret-key\") \\\n    .config(\"spark.sql.catalog.lance.root\", \"s3://your-bucket/lance\") \\\n    .getOrCreate()\n</code></pre> <pre><code>val spark = SparkSession.builder()\n    .appName(\"lance-glue-example\")\n    .config(\"spark.sql.catalog.lance\", \"com.lancedb.lance.spark.LanceNamespaceSparkCatalog\")\n    .config(\"spark.sql.catalog.lance.impl\", \"glue\")\n    .config(\"spark.sql.catalog.lance.region\", \"us-east-1\")\n    .config(\"spark.sql.catalog.lance.catalog_id\", \"123456789012\")\n    .config(\"spark.sql.catalog.lance.access_key_id\", \"your-access-key\")\n    .config(\"spark.sql.catalog.lance.secret_access_key\", \"your-secret-key\")\n    .config(\"spark.sql.catalog.lance.root\", \"s3://your-bucket/lance\")\n    .getOrCreate()\n</code></pre> <pre><code>SparkSession spark = SparkSession.builder()\n    .appName(\"lance-glue-example\")\n    .config(\"spark.sql.catalog.lance\", \"com.lancedb.lance.spark.LanceNamespaceSparkCatalog\")\n    .config(\"spark.sql.catalog.lance.impl\", \"glue\")\n    .config(\"spark.sql.catalog.lance.region\", \"us-east-1\")\n    .config(\"spark.sql.catalog.lance.catalog_id\", \"123456789012\")\n    .config(\"spark.sql.catalog.lance.access_key_id\", \"your-access-key\")\n    .config(\"spark.sql.catalog.lance.secret_access_key\", \"your-secret-key\")\n    .config(\"spark.sql.catalog.lance.root\", \"s3://your-bucket/lance\")\n    .getOrCreate();\n</code></pre>"},{"location":"user-guide/config/#glue-configuration-parameters","title":"Glue Configuration Parameters","text":"Parameter Required Description <code>spark.sql.catalog.{name}.region</code> \u2713 AWS region for Glue operations (e.g., <code>us-east-1</code>) <code>spark.sql.catalog.{name}.catalog_id</code> \u2717 Glue catalog ID, defaults to the AWS account ID of the caller <code>spark.sql.catalog.{name}.endpoint</code> \u2717 Custom Glue service endpoint for connecting to compatible metastores <code>spark.sql.catalog.{name}.access_key_id</code> \u2717 AWS access key ID for static credentials <code>spark.sql.catalog.{name}.secret_access_key</code> \u2717 AWS secret access key for static credentials <code>spark.sql.catalog.{name}.session_token</code> \u2717 AWS session token for temporary credentials <code>spark.sql.catalog.{name}.root</code> \u2717 Storage root location (e.g., <code>s3://bucket/path</code>), defaults to current directory <code>spark.sql.catalog.{name}.storage.*</code> \u2717 Additional storage configuration options"},{"location":"user-guide/config/#apache-hive-namespace","title":"Apache Hive Namespace","text":"<p>Lance supports both Hive 2.x and Hive 3.x metastores for metadata management:</p>"},{"location":"user-guide/config/#hive-3x-namespace","title":"Hive 3.x Namespace","text":"PySparkScalaJava <pre><code>spark = SparkSession.builder \\\n    .appName(\"lance-hive3-example\") \\\n    .config(\"spark.sql.catalog.lance\", \"com.lancedb.lance.spark.LanceNamespaceSparkCatalog\") \\\n    .config(\"spark.sql.catalog.lance.impl\", \"hive3\") \\\n    .config(\"spark.sql.catalog.lance.parent\", \"hive\") \\\n    .config(\"spark.sql.catalog.lance.parent_delimiter\", \".\") \\\n    .config(\"spark.sql.catalog.lance.hadoop.hive.metastore.uris\", \"thrift://metastore:9083\") \\\n    .config(\"spark.sql.catalog.lance.client.pool-size\", \"5\") \\\n    .config(\"spark.sql.catalog.lance.root\", \"hdfs://namenode:8020/lance\") \\\n    .getOrCreate()\n</code></pre> <pre><code>val spark = SparkSession.builder()\n    .appName(\"lance-hive3-example\")\n    .config(\"spark.sql.catalog.lance\", \"com.lancedb.lance.spark.LanceNamespaceSparkCatalog\")\n    .config(\"spark.sql.catalog.lance.impl\", \"hive3\")\n    .config(\"spark.sql.catalog.lance.parent\", \"hive\")\n    .config(\"spark.sql.catalog.lance.parent_delimiter\", \".\")\n    .config(\"spark.sql.catalog.lance.hadoop.hive.metastore.uris\", \"thrift://metastore:9083\")\n    .config(\"spark.sql.catalog.lance.client.pool-size\", \"5\")\n    .config(\"spark.sql.catalog.lance.root\", \"hdfs://namenode:8020/lance\")\n    .getOrCreate()\n</code></pre> <pre><code>SparkSession spark = SparkSession.builder()\n    .appName(\"lance-hive3-example\")\n    .config(\"spark.sql.catalog.lance\", \"com.lancedb.lance.spark.LanceNamespaceSparkCatalog\")\n    .config(\"spark.sql.catalog.lance.impl\", \"hive3\")\n    .config(\"spark.sql.catalog.lance.parent\", \"hive\")\n    .config(\"spark.sql.catalog.lance.parent_delimiter\", \".\")\n    .config(\"spark.sql.catalog.lance.hadoop.hive.metastore.uris\", \"thrift://metastore:9083\")\n    .config(\"spark.sql.catalog.lance.client.pool-size\", \"5\")\n    .config(\"spark.sql.catalog.lance.root\", \"hdfs://namenode:8020/lance\")\n    .getOrCreate();\n</code></pre>"},{"location":"user-guide/config/#hive-2x-namespace","title":"Hive 2.x Namespace","text":"PySparkScalaJava <pre><code>spark = SparkSession.builder \\\n    .appName(\"lance-hive2-example\") \\\n    .config(\"spark.sql.catalog.lance\", \"com.lancedb.lance.spark.LanceNamespaceSparkCatalog\") \\\n    .config(\"spark.sql.catalog.lance.impl\", \"hive2\") \\\n    .config(\"spark.sql.catalog.lance.hadoop.hive.metastore.uris\", \"thrift://metastore:9083\") \\\n    .config(\"spark.sql.catalog.lance.client.pool-size\", \"3\") \\\n    .config(\"spark.sql.catalog.lance.root\", \"hdfs://namenode:8020/lance\") \\\n    .getOrCreate()\n</code></pre> <pre><code>val spark = SparkSession.builder()\n    .appName(\"lance-hive2-example\")\n    .config(\"spark.sql.catalog.lance\", \"com.lancedb.lance.spark.LanceNamespaceSparkCatalog\")\n    .config(\"spark.sql.catalog.lance.impl\", \"hive2\")\n    .config(\"spark.sql.catalog.lance.hadoop.hive.metastore.uris\", \"thrift://metastore:9083\")\n    .config(\"spark.sql.catalog.lance.client.pool-size\", \"3\")\n    .config(\"spark.sql.catalog.lance.root\", \"hdfs://namenode:8020/lance\")\n    .getOrCreate()\n</code></pre> <pre><code>SparkSession spark = SparkSession.builder()\n    .appName(\"lance-hive2-example\")\n    .config(\"spark.sql.catalog.lance\", \"com.lancedb.lance.spark.LanceNamespaceSparkCatalog\")\n    .config(\"spark.sql.catalog.lance.impl\", \"hive2\")\n    .config(\"spark.sql.catalog.lance.hadoop.hive.metastore.uris\", \"thrift://metastore:9083\")\n    .config(\"spark.sql.catalog.lance.client.pool-size\", \"3\")\n    .config(\"spark.sql.catalog.lance.root\", \"hdfs://namenode:8020/lance\")\n    .getOrCreate();\n</code></pre>"},{"location":"user-guide/config/#hive-configuration-parameters","title":"Hive Configuration Parameters","text":"Parameter Required Description <code>spark.sql.catalog.{name}.hadoop.*</code> \u2717 Additional Hadoop configuration options, will override the default Hadoop configuration <code>spark.sql.catalog.{name}.client.pool-size</code> \u2717 Connection pool size for metastore clients (default: 3) <code>spark.sql.catalog.{name}.root</code> \u2717 Storage root location for Lance tables (default: current directory) <code>spark.sql.catalog.{name}.storage.*</code> \u2717 Additional storage configuration options <code>spark.sql.catalog.{name}.parent</code> \u2717 Parent prefix for multi-level namespaces (Hive 3.x only, default: <code>hive</code>)"},{"location":"user-guide/config/#note-on-namespace-levels","title":"Note on Namespace Levels","text":"<p>Spark provides at least a 3 level hierarchy of catalog \u2192 multi-level namespace \u2192 table Most users treat Spark as a 3 level hierarchy with 1 level namespace.</p>"},{"location":"user-guide/config/#for-namespaces-with-less-than-3-levels","title":"For Namespaces with Less Than 3 Levels","text":"<p>Since Lance allows a 2 level hierarchy of root namespace \u2192 table for namespaces like <code>DirectoryNamespace</code>, the <code>LanceNamespaceSparkCatalog</code> provides a configuration <code>extra_level</code> which puts an additional dummy level to match the Spark hierarchy and make it root namespace \u2192 dummy extra level \u2192 table.</p> <p>Currently, this is automatically set with <code>extra_level=default</code> for <code>DirectoryNamespace</code> and when <code>RestNamespace</code> if it cannot respond to <code>ListNamespaces</code> operation. If you have a custom namespace implementation of the same behavior, you can also set the config to add the extra level.</p>"},{"location":"user-guide/config/#for-namespaces-with-more-than-3-levels","title":"For Namespaces with More Than 3 Levels","text":"<p>Some namespace implementations like Hive3 support more than 3 levels of hierarchy. For example, Hive3 has a  4 level hierarchy: root metastore \u2192 catalog \u2192 database \u2192 table.</p> <p>To handle this, the <code>LanceNamespaceSparkCatalog</code> provides <code>parent</code> and <code>parent_delimiter</code> configurations which allow you to specify a parent prefix that gets prepended to all namespace operations.</p> <p>For example, with Hive3: - Setting <code>parent=hive</code> and <code>parent_delimiter=.</code>  - When Spark requests namespace <code>[\"database1\"]</code>, it gets transformed to <code>[\"hive\", \"database1\"]</code> for the API call - This allows the 4-level Hive3 structure to work within Spark's 3-level model</p> <p>The parent configuration effectively \"anchors\" your Spark catalog at a specific level within the deeper namespace hierarchy, making the extra levels transparent to Spark users while maintaining compatibility with the underlying namespace implementation.</p>"},{"location":"user-guide/install/","title":"Install","text":""},{"location":"user-guide/install/#maven-central-packages","title":"Maven Central Packages","text":"<p>The connector packages are published to Maven Central under the <code>com.lancedb</code> namespace. Choose the appropriate artifact based on your use case:</p>"},{"location":"user-guide/install/#available-artifacts","title":"Available Artifacts","text":"Artifact Type Name Pattern Description Example Base Jar <code>lance-spark-base_&lt;scala_version&gt;</code> Jar with logic shared by different versions of Spark Lance connectors. lance-spark-base_2.12 Lean Jar <code>lance-spark-&lt;spark-version&gt;_&lt;scala_version&gt;</code> Jar with only the Spark Lance connector logic lance-spark-3.5_2.12 Bundled Jar <code>lance-spark-bundle-&lt;spark-version&gt;_&lt;scala_version&gt;</code> Jar with all necessary non-Spark dependencies lance-spark-bundle-3.5_2.12"},{"location":"user-guide/install/#choosing-the-right-artifact","title":"Choosing the Right Artifact","text":"<ul> <li>Bundled Jar: Recommended for most users. Use this if you want to quickly get started or use the connector in a Spark shell/notebook environment.</li> <li>Lean Jar: Use this if you're building a custom Spark application and want to manage and bundle dependencies yourself.</li> <li>Base Jar: Internal use only. Use this if you would like to build a custom Spark Lance connector with a different Spark version.</li> </ul>"},{"location":"user-guide/install/#dependency-configuration","title":"Dependency Configuration","text":""},{"location":"user-guide/install/#in-spark-application-code","title":"In Spark Application Code","text":"<p>Typically, you use the bundled jar in your Spark application as a provided (compile only) dependency. The actual jar is supplied to the Spark cluster separately. If you want to also include the bundled jar in your own bundle, remove the provided (compile only) annotation.</p> MavenGradlesbt <pre><code>&lt;!-- For Spark 3.5 (Scala 2.12) --&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;com.lancedb&lt;/groupId&gt;\n    &lt;artifactId&gt;lance-spark-bundle-3.5_2.12&lt;/artifactId&gt;\n    &lt;version&gt;0.0.5&lt;/version&gt;\n    &lt;scope&gt;provided&lt;/scope&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>dependencies {\n    // For Spark 3.5 (Scala 2.12)\n    compileOnly 'com.lancedb:lance-spark-bundle-3.5_2.12:0.0.5'\n}\n</code></pre> <pre><code>libraryDependencies ++= Seq(\n  // For Spark 3.5 (Scala 2.12)\n  \"com.lancedb\" %% \"lance-spark-bundle-3.5_2.12\" % \"0.0.5\" % \"provided\"\n)\n</code></pre>"},{"location":"user-guide/install/#in-spark-cluster","title":"In Spark Cluster","text":"<p>You can either download the bundled jar dependency from Maven and add it to your Spark classpath, or supply the dependency dynamically to a Spark cluster through <code>--packages</code> flag. See Configuration for more details.</p>"},{"location":"user-guide/install/#requirements","title":"Requirements","text":""},{"location":"user-guide/install/#java","title":"Java","text":"Java Version Support Status Notes Java 8 \u2705 Supported Minimum required version Java 11 \u2705 Supported Recommended for production Java 17 \u2705 Supported Latest LTS version (Spark 4.0 dropped Java 8 and 11 support) Java 21+ \u26a0\ufe0f Untested May work but not officially tested"},{"location":"user-guide/install/#scala","title":"Scala","text":"Scala Version Support Status Notes Scala 2.12 \u2705 Supported Fully supported Scala 2.13 \u2705 Supported Fully supported Scala 3.x \u274c Not Supported Not currently planned"},{"location":"user-guide/install/#apache-spark","title":"Apache Spark","text":"Spark Version Support Status Notes Spark 4.0 \u2705 Supported Scala 2.13 only (Spark 4.0 dropped Scala 2.12 support) Spark 3.5 \u2705 Supported Scala 2.12 and 2.13 Spark 3.4 \u2705 Supported Scala 2.12 and 2.13 Spark 3.1, 3.2, 3.3 \u26a0\ufe0f Untested May work but not officially tested Spark 2.x \u274c Not Supported"},{"location":"user-guide/install/#operating-system","title":"Operating System","text":"Operating System Architecture Support Status Notes Linux x86_64 \u2705 Supported Linux ARM64 \u2705 Supported Including Apple Silicon via Rosetta macOS x86_64 \u2705 Supported Intel-based Macs macOS ARM64 \u2705 Supported Apple Silicon (M1/M2/M3) Windows x86_64 \ud83d\udea7 In Progress Support planned for future releases"},{"location":"user-guide/read/","title":"Inspecting your Lance Datasets","text":""},{"location":"user-guide/read/#select","title":"SELECT","text":"<pre><code>-- Select all data from a table\nSELECT * FROM users;\n\n-- Select specific columns\nSELECT id, name, email FROM users;\n\n-- Query with WHERE clause\nSELECT * FROM users WHERE age &gt; 25;\n\n-- Aggregate queries\nSELECT department, COUNT(*) as employee_count \nFROM users \nGROUP BY department;\n\n-- Join queries\nSELECT u.name, p.title\nFROM users u\nJOIN projects p ON u.id = p.user_id;\n</code></pre>"},{"location":"user-guide/read/#show-tables","title":"SHOW TABLES","text":"<pre><code>-- Show all tables in the default namespace\nSHOW TABLES;\n\n-- Show all tables in a specific namespace ns2\n SNOW TABLES IN ns2;\n</code></pre>"},{"location":"user-guide/read/#describe-table","title":"DESCRIBE TABLE","text":"<pre><code>-- Describe table structure\nDESCRIBE TABLE users;\n\n-- Show detailed table information\nDESCRIBE EXTENDED users;\n</code></pre>"},{"location":"user-guide/read/#dataframe-read","title":"DataFrame Read","text":"PythonScalaJava <pre><code># Load table as DataFrame\nusers_df = spark.table(\"lance.default.users\")\n\n# Use DataFrame operations\nfiltered_users = users_df.filter(\"age &gt; 25\").select(\"name\", \"email\")\nfiltered_users.show()\n</code></pre> <pre><code>// Load table as DataFrame\nval usersDF = spark.table(\"lance.default.users\")\n\n// Use DataFrame operations\nval filteredUsers = usersDF.filter(\"age &gt; 25\").select(\"name\", \"email\")\nfilteredUsers.show()\n</code></pre> <pre><code>// Load table as DataFrame\nDataset&lt;Row&gt; usersDF = spark.table(\"lance.default.users\");\n\n// Use DataFrame operations\nDataset&lt;Row&gt; filteredUsers = usersDF.filter(\"age &gt; 25\").select(\"name\", \"email\");\nfilteredUsers.show();\n</code></pre>"},{"location":"user-guide/write/","title":"Managing your Lance Datasets","text":""},{"location":"user-guide/write/#create-table","title":"CREATE TABLE","text":"<pre><code>-- Create a simple table\nCREATE TABLE users (\n    id BIGINT NOT NULL,\n    name STRING,\n    email STRING,\n    created_at TIMESTAMP\n);\n\n\n-- Create table with complex data types\nCREATE TABLE events (\n    event_id BIGINT NOT NULL,\n    user_id BIGINT,\n    event_type STRING,\n    tags ARRAY&lt;STRING&gt;,\n    metadata STRUCT&lt;\n        source: STRING,\n        version: INT,\n        processed_at: TIMESTAMP\n    &gt;,\n    occurred_at TIMESTAMP\n);\n</code></pre>"},{"location":"user-guide/write/#insert-into","title":"INSERT INTO","text":"<pre><code>-- Insert individual rows\nINSERT INTO users VALUES \n    (4, 'David', 'david@example.com', '2024-01-15 10:30:00'),\n    (5, 'Eva', 'eva@example.com', '2024-01-15 11:45:00');\n\n-- Insert with column specification\nINSERT INTO users (id, name, email) VALUES \n    (6, 'Frank', 'frank@example.com'),\n    (7, 'Grace', 'grace@example.com');\n\n-- Insert from SELECT query\nINSERT INTO users\nSELECT user_id as id, username as name, email_address as email, signup_date as created_at\nFROM staging.user_signups\nWHERE signup_date &gt;= '2024-01-01';\n\n-- Insert with complex data types\nINSERT INTO events VALUES (\n    1001,\n    123,\n    'page_view',\n    map('page', '/home', 'referrer', 'google'),\n    array('web', 'desktop'),\n    struct('web_app', 1, '2024-01-15 12:00:00'),\n    '2024-01-15 12:00:00'\n);\n</code></pre>"},{"location":"user-guide/write/#drop-table","title":"DROP TABLE","text":"<pre><code>-- Drop table\nDROP TABLE users;\n\n-- Drop table if it exists (no error if table doesn't exist)\nDROP TABLE IF EXISTS users;\n</code></pre>"},{"location":"user-guide/write/#dataframe-createtable","title":"DataFrame CreateTable","text":"PythonScalaJava <pre><code># Create DataFrame\ndata = [\n(1, \"Alice\", \"alice@example.com\"),\n(2, \"Bob\", \"bob@example.com\"),\n(3, \"Charlie\", \"charlie@example.com\")\n]\ndf = spark.createDataFrame(data, [\"id\", \"name\", \"email\"])\n\n# Write as new table using catalog\ndf.writeTo(\"users\").create()\n</code></pre> <pre><code>import spark.implicits._\n\n// Create DataFrame\nval data = Seq(\n    (1, \"Alice\", \"alice@example.com\"),\n    (2, \"Bob\", \"bob@example.com\"),\n    (3, \"Charlie\", \"charlie@example.com\")\n)\nval df = data.toDF(\"id\", \"name\", \"email\")\n\n// Write as new table using catalog\ndf.writeTo(\"users\").create()\n</code></pre> <pre><code>import org.apache.spark.sql.types.*;\nimport org.apache.spark.sql.Row;\nimport org.apache.spark.sql.RowFactory;\n\n// Create DataFrame\nList&lt;Row&gt; data = Arrays.asList(\n    RowFactory.create(1L, \"Alice\", \"alice@example.com\"),\n    RowFactory.create(2L, \"Bob\", \"bob@example.com\"),\n    RowFactory.create(3L, \"Charlie\", \"charlie@example.com\")\n);\n\nStructType schema = new StructType(new StructField[]{\n    new StructField(\"id\", DataTypes.LongType, false, Metadata.empty()),\n    new StructField(\"name\", DataTypes.StringType, true, Metadata.empty()),\n    new StructField(\"email\", DataTypes.StringType, true, Metadata.empty())\n});\n\nDataset&lt;Row&gt; df = spark.createDataFrame(data, schema);\n\n// Write as new table using catalog\ndf.writeTo(\"users\").create();\n</code></pre>"},{"location":"user-guide/write/#dataframe-write","title":"DataFrame Write","text":"PythonScalaJava <pre><code># Create new data\nnew_data = [\n    (8, \"Henry\", \"henry@example.com\"),\n    (9, \"Ivy\", \"ivy@example.com\")\n]\nnew_df = spark.createDataFrame(new_data, [\"id\", \"name\", \"email\"])\n\n# Append to existing table\nnew_df.writeTo(\"users\").append()\n\n# Alternative: use traditional write API with mode\nnew_df.write.mode(\"append\").saveAsTable(\"users\")\n</code></pre> <pre><code>// Create new data\nval newData = Seq(\n    (8, \"Henry\", \"henry@example.com\"),\n    (9, \"Ivy\", \"ivy@example.com\")\n)\nval newDF = newData.toDF(\"id\", \"name\", \"email\")\n\n// Append to existing table\nnewDF.writeTo(\"users\").append()\n\n// Alternative: use traditional write API with mode\nnewDF.write.mode(\"append\").saveAsTable(\"users\")\n</code></pre> <pre><code>// Create new data\nList&lt;Row&gt; newData = Arrays.asList(\n    RowFactory.create(8L, \"Henry\", \"henry@example.com\"),\n    RowFactory.create(9L, \"Ivy\", \"ivy@example.com\")\n);\nDataset&lt;Row&gt; newDF = spark.createDataFrame(newData, schema);\n\n// Append to existing table\nnewDF.writeTo(\"users\").append();\n\n// Alternative: use traditional write API with mode\nnewDF.write().mode(\"append\").saveAsTable(\"users\");\n</code></pre>"}]}